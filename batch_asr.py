from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
import soundfile as sf
import os
import pandas as pd
from tqdm import tqdm  # ç”¨äºè¿›åº¦æ˜¾ç¤º

# æ¨¡å‹è·¯å¾„
model_dir = "SenseVoiceSmall"
vad_model_dir = "fsmn-vad"  # VADæ¨¡å‹è·¯å¾„

# éŸ³é¢‘æ–‡ä»¶å¤¹è·¯å¾„
audio_folder = "extracted_audio"
output_folder = "transcriptions"
os.makedirs(output_folder, exist_ok=True)

# åŠ è½½VADæ¨¡å‹
vad_model = AutoModel(
    model=vad_model_dir,
    trust_remote_code=True,
    remote_code="./model.py",
    device="cuda:1",
    disable_update=True
)

# åŠ è½½SenseVoiceæ¨¡å‹
model = AutoModel(
    model=model_dir,
    trust_remote_code=True,
    remote_code="./model.py",
    device="cuda:1",
    disable_update=True
)

# å®šä¹‰è£å‰ªéŸ³é¢‘å‡½æ•°
def crop_audio(audio_data, start_time, end_time, sample_rate):
    start_sample = int(start_time * sample_rate / 1000)  # è½¬æ¢ä¸ºæ ·æœ¬æ•°
    end_sample = int(end_time * sample_rate / 1000)  # è½¬æ¢ä¸ºæ ·æœ¬æ•°
    return audio_data[start_sample:end_sample]

# è·å–æ‰€æœ‰WAVæ–‡ä»¶
audio_files = [f for f in os.listdir(audio_folder) if f.endswith('.wav')]

# è¿›åº¦æ¡
with tqdm(total=len(audio_files), desc="Processing audio files") as pbar:
    for audio_file in audio_files:
        try:
            audio_path = os.path.join(audio_folder, audio_file)

            # ä½¿ç”¨VADæ¨¡å‹å¤„ç†éŸ³é¢‘æ–‡ä»¶
            vad_res = vad_model.generate(
                input=audio_path,
                cache={},
                max_single_segment_time=30000,
            )
            # ä»VADæ¨¡å‹çš„è¾“å‡ºä¸­æå–æ¯ä¸ªè¯­éŸ³ç‰‡æ®µçš„å¼€å§‹å’Œç»“æŸæ—¶é—´
            segments = vad_res[0]['value']

            # åŠ è½½åŸå§‹éŸ³é¢‘æ•°æ®
            audio_data, sample_rate = sf.read(audio_path)

            results = []
            for idx, segment in enumerate(segments):
                start_time, end_time = segment
                cropped_audio = crop_audio(audio_data, start_time, end_time, sample_rate)

                # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                temp_audio_file = f"temp_{idx}.wav"
                sf.write(temp_audio_file, cropped_audio, sample_rate)

                # è¯­éŸ³è½¬æ–‡å­—å¤„ç†
                res = model.generate(
                    input=temp_audio_file,
                    cache={},
                    language="en",
                    use_itn=True,
                    batch_size_s=60,
                    merge_vad=True,
                    merge_length_s=10000,
                )

                # åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
                os.remove(temp_audio_file)

                # å¤„ç†è¾“å‡ºç»“æœ
                text = rich_transcription_postprocess(res[0]["text"])
                # results.append({"start": start_time // 1000, "end": end_time // 1000, "text": text})
                results.append({"start": round(start_time / 1000, 3), "end": round(end_time / 1000, 3), "text": text})


            # å°†ç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶
            output_file = os.path.join(output_folder, f"{os.path.splitext(audio_file)[0]}.csv")
            pd.DataFrame(results).to_csv(output_file, index=False)
            print(f"âœ… å®Œæˆ: {audio_file} -> {output_file}")

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {audio_file}, é”™è¯¯: {e}")

        # æ›´æ–°è¿›åº¦æ¡
        pbar.update(1)

print("ğŸš€ æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼")
